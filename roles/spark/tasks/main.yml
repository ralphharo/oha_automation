---
# SPARK SETUP

# deploy spark   
   - unarchive: "src={{ oha_artifacts }}/{{ artifact }} dest={{ oha_home }} copy=no owner={{ orion_user }} group={{ orion_group }} mode=0755"

   - file: path=/tmp/spark-events state=directory mode=0777 

# spark config files
   - name: Copy spark-defaults.conf from template
     command: "cp {{ spark_home }}/conf/spark-defaults.conf.template {{ spark_home }}/conf/spark-defaults.conf"
 
   - name: Copy slaves from template
     command: "cp {{ spark_home }}/conf/slaves.template {{ spark_home }}/conf/slaves"
     
   - name: Include spark-env template
     template: 
       src: "roles/spark/templates/spark-env.sh"
       dest: "{{ spark_home }}/conf"    

   - lineinfile: dest=/etc/environment regexp='^export SPARK_HOME=' line='export SPARK_HOME="/opt/spark-1.6.0-bin-hadoop2.4/"'
 
 # Update Spark Master configuration   
   - name: Update master with slaves with spark URL 
     lineinfile: dest={{ spark_home }}/conf/slaves line="{{ item }}" insertafter=EOF
     with_file: roles/spark/files/slaveIp.yml
     when: 
       node_type == "master"
     
   - name: Update spark-defaults.conf with spark URL - ALL NODES
     lineinfile: dest={{ spark_home }}/conf/spark-defaults.conf line="spark://{{ masterPrivateIp }}:7077"
  
 # Update Spark master/worker configuration       
   - name: Include master init file
     template:
       src: "roles/spark/templates/spark-master"
       dest: "/etc/init.d"
       mode: 0755
     when: 
       node_type == "master"       

   - name: Include worker init file
     template:
       src: "roles/spark/templates/spark-worker"
       dest: "/etc/init.d"
       mode: 0755
     when: 
       node_type == "worker"       

 # Restart Spark master/worker
   - name: start master
     service: name=spark-master state=restarted
     when: 
       node_type == "master"
       
   - name: start worker
     service: name=spark-worker state=restarted
     when: 
       node_type == "worker"
                 
